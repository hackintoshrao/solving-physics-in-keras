{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "physics_problem_keras",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "/v2/external/notebooks/gpu.ipynb",
          "timestamp": 1519195961597
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BlmQIFSLZDdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural nets to solve a physics problem\n"
      ]
    },
    {
      "metadata": {
        "id": "8TYFoOk6PsTd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 57
            },
            {
              "item_id": 98
            },
            {
              "item_id": 187
            },
            {
              "item_id": 218
            },
            {
              "item_id": 219
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 4014
        },
        "outputId": "54e916d0-17c2-4b43-cfe4-f2e94d86587c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519375619744,
          "user_tz": -330,
          "elapsed": 149708,
          "user": {
            "displayName": "karthic rao",
            "photoUrl": "//lh4.googleusercontent.com/-IjQPV2IT_dg/AAAAAAAAAAI/AAAAAAAAAKY/koCnnupHc0Y/s50-c-k-no/photo.jpg",
            "userId": "117034387844131328042"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "from os import path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.cross_validation import train_test_split\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install tqdm\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm, trange\n",
        "def V(x):   # converts a numpy array to a pyTorch variable\n",
        "    return Variable(torch.FloatTensor(np.array(x)), requires_grad=True)\n",
        "def C(x):   # same as above; but this is for y0 (the actual value), not y (the NN-output value); pyTorch actually forces me to do this...\n",
        "    return Variable(torch.FloatTensor(np.array(x)), requires_grad=False)\n",
        "def N(x):   # converts back\n",
        "    return x.data.cpu().numpy()\n",
        " \n",
        "Nsamples = 20000\n",
        "max_Nfeatures = 100\n",
        "epochs = 300\n",
        " \n",
        "# generate data\n",
        "Xs = []\n",
        "ys = []\n",
        "for isample in trange(Nsamples):\n",
        "    # a random number of features\n",
        "    Nfeatures = np.random.randint(max_Nfeatures-40) + 40\n",
        " \n",
        "    # generate X\n",
        "    # intermediateX example: shape [50, 3], content [[-25,25,44], [-10,30,40], ....]\n",
        "    intermediateX = np.random.rand(Nfeatures, 3) * 50 - 50 * np.random.rand(3)\n",
        "    # X example: shape [2500, 6], content [[-25,25,44,-25,25,44], [-25,25,44,-10,30,40], ...]\n",
        "    X = np.zeros((Nfeatures, Nfeatures, 6))\n",
        "    X[:,:,:3] = np.expand_dims(intermediateX, 1)\n",
        "    X[:,:,3:] = np.expand_dims(intermediateX, 0)\n",
        "    X = X.reshape(-1,6)\n",
        "    X = X * np.mgrid[1:2:len(X)]   # a modification that would seem meaningless from your point of view... just keep it...\n",
        "    # Note: all these is similar to below; but I found my data-generating method to generate much-harder-to-train data\n",
        "    # X = np.random.rand(Nfeatures * Nfeatures, 6)\n",
        " \n",
        "    # generate y\n",
        "    origin = V([0,0,0])\n",
        "    # firstly, z=f(x)\n",
        "    X1 = V(X[:,:3]) - origin   # example shape: (2500, 3); origin=0 so ignore origin for now\n",
        "    X2 = V(X[:,3:]) - origin\n",
        "    R1 = torch.norm(X1, dim=1)       # example shape: (2500, 1)\n",
        "    R2 = torch.norm(X2, dim=1)\n",
        "    z = 1 / R1 / R2      # example shape: (2500, 1)\n",
        "    z = torch.sum(z) * 1e4        # example shape: 1\n",
        "    # then y=g(z). why not just ask you to train z? because training y is much more difficult than z.\n",
        "    y = N(torch.autograd.grad(z, origin, create_graph=True)[0])\n",
        " \n",
        "    Xs.append(np.mean(X, 0))\n",
        "    ys.append(y / (len(X)))\n",
        "     \n",
        "\n",
        "print(\"\\n\")\n",
        "print(Xs[0].shape)\n",
        "print(y.shape)\n",
        "print(y)\n",
        "print(X[0])\n",
        "print(Xs[0])\n",
        "print(ys[0])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, train_size = 0.8)\n",
        "print(type(X_train))\n",
        "\n",
        "print(\"train\")\n",
        "print(len(X_train))\n",
        "print(X_train[0].shape)\n",
        "model = Sequential()\n",
        "model.add(Dense(units=400, input_dim=6))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units=200))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units=100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=50))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=25))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units=3))\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd',metrics=['accuracy'])\n",
        "\n",
        "model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=50, verbose=2)\n",
        "\n",
        "loss_and_metrics = model.evaluate(np.array(X_test), np.array(y_test), batch_size=100)\n",
        "\n",
        "classes = model.predict(np.array(X_test), batch_size=1)\n",
        "\n",
        "print(classes[:10])\n",
        "print(y_test[:10])\n",
        "\"\"\"\n",
        "# neural net\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(6, 192),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(192, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1)\n",
        ")\n",
        "optimizer = optim.Adam(net.parameters(), lr=1E-4)\n",
        "criterion = nn.MSELoss()\n",
        "net.train()\n",
        "\n",
        "# train\n",
        "for epoch in trange(epochs * Nsamples): # using trange instead of range produces a progressbar\n",
        "    # randomly select a sample\n",
        "    isample = np.random.randint(0, Nsamples)\n",
        "    y0 = C(ys[isample])\n",
        " \n",
        "    # calculate neural net output\n",
        "    X = V(Xs[isample]) - torch.cat([origin, origin])\n",
        "    z = torch.sum(net(X))\n",
        "    # z = torch.sum(1 / torch.norm(X[:,:3], dim=1) / torch.norm(X[:,3:], dim=1) * 1e4)  # this is 100% accurate\n",
        "    y = torch.autograd.grad(z, origin, create_graph=True)[0]\n",
        " \n",
        "    # and perform train step\n",
        "    loss = criterion(y,y0)\n",
        "    optimizer.zero_grad()   # suggested trick\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        " \n",
        "    if epoch % 1000 == 0:\n",
        "        tqdm.write('%s\\t%s\\t%s\\t%s' %(epoch, N(loss)[0], N(y0)[0], N(y)[0]))\n",
        "\"\"\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:39<00:00, 502.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "(6,)\n",
            "(3,)\n",
            "[-2906.2344   -915.64734 -1517.0038 ]\n",
            "[-15.84931198  15.02764017 -10.42799311 -15.84931198  15.02764017\n",
            " -10.42799311]\n",
            "[ 18.19428478   6.20820177 -11.25640949  18.19428478   6.20820177\n",
            " -11.25640949]\n",
            "[ 0.69157505  0.30031165 -0.09098879]\n",
            "<type 'list'>\n",
            "train\n",
            "16000\n",
            "(6,)\n",
            "Epoch 1/100\n",
            " - 1s - loss: 1.9295 - acc: 0.5561\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.8847 - acc: 0.6228\n",
            "Epoch 3/100\n",
            " - 1s - loss: 1.8742 - acc: 0.6557\n",
            "Epoch 4/100\n",
            " - 1s - loss: 1.8682 - acc: 0.6699\n",
            "Epoch 5/100\n",
            " - 1s - loss: 1.8658 - acc: 0.6744\n",
            "Epoch 6/100\n",
            " - 1s - loss: 1.8635 - acc: 0.6800\n",
            "Epoch 7/100\n",
            " - 1s - loss: 1.8624 - acc: 0.6855\n",
            "Epoch 8/100\n",
            " - 1s - loss: 1.8624 - acc: 0.6888\n",
            "Epoch 9/100\n",
            " - 1s - loss: 1.8603 - acc: 0.6919\n",
            "Epoch 10/100\n",
            " - 1s - loss: 1.8583 - acc: 0.6903\n",
            "Epoch 11/100\n",
            " - 1s - loss: 1.8612 - acc: 0.6939\n",
            "Epoch 12/100\n",
            " - 1s - loss: 1.8552 - acc: 0.6963\n",
            "Epoch 13/100\n",
            " - 1s - loss: 1.8568 - acc: 0.7053\n",
            "Epoch 14/100\n",
            " - 1s - loss: 1.8577 - acc: 0.7022\n",
            "Epoch 15/100\n",
            " - 1s - loss: 1.8527 - acc: 0.7085\n",
            "Epoch 16/100\n",
            " - 1s - loss: 1.8574 - acc: 0.7048\n",
            "Epoch 17/100\n",
            " - 1s - loss: 1.8563 - acc: 0.7099\n",
            "Epoch 18/100\n",
            " - 1s - loss: 1.8549 - acc: 0.7025\n",
            "Epoch 19/100\n",
            " - 1s - loss: 1.8549 - acc: 0.7103\n",
            "Epoch 20/100\n",
            " - 1s - loss: 1.8505 - acc: 0.7076\n",
            "Epoch 21/100\n",
            " - 1s - loss: 1.8516 - acc: 0.7133\n",
            "Epoch 22/100\n",
            " - 1s - loss: 1.8545 - acc: 0.7099\n",
            "Epoch 23/100\n",
            " - 1s - loss: 1.8509 - acc: 0.7053\n",
            "Epoch 24/100\n",
            " - 1s - loss: 1.8505 - acc: 0.7077\n",
            "Epoch 25/100\n",
            " - 1s - loss: 1.8490 - acc: 0.7098\n",
            "Epoch 26/100\n",
            " - 1s - loss: 1.8507 - acc: 0.7114\n",
            "Epoch 27/100\n",
            " - 1s - loss: 1.8515 - acc: 0.7148\n",
            "Epoch 28/100\n",
            " - 1s - loss: 1.8501 - acc: 0.7151\n",
            "Epoch 29/100\n",
            " - 1s - loss: 1.8489 - acc: 0.7141\n",
            "Epoch 30/100\n",
            " - 1s - loss: 1.8491 - acc: 0.7083\n",
            "Epoch 31/100\n",
            " - 1s - loss: 1.8494 - acc: 0.7123\n",
            "Epoch 32/100\n",
            " - 1s - loss: 1.8488 - acc: 0.7071\n",
            "Epoch 33/100\n",
            " - 1s - loss: 1.8527 - acc: 0.7154\n",
            "Epoch 34/100\n",
            " - 1s - loss: 1.8483 - acc: 0.7139\n",
            "Epoch 35/100\n",
            " - 1s - loss: 1.8485 - acc: 0.7089\n",
            "Epoch 36/100\n",
            " - 1s - loss: 1.8500 - acc: 0.7183\n",
            "Epoch 37/100\n",
            " - 1s - loss: 1.8474 - acc: 0.7125\n",
            "Epoch 38/100\n",
            " - 1s - loss: 1.8482 - acc: 0.7152\n",
            "Epoch 39/100\n",
            " - 1s - loss: 1.8482 - acc: 0.7216\n",
            "Epoch 40/100\n",
            " - 1s - loss: 1.8463 - acc: 0.7190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/100\n",
            " - 1s - loss: 1.8467 - acc: 0.7154\n",
            "Epoch 42/100\n",
            " - 1s - loss: 1.8510 - acc: 0.7134\n",
            "Epoch 43/100\n",
            " - 1s - loss: 1.8463 - acc: 0.7174\n",
            "Epoch 44/100\n",
            " - 1s - loss: 1.8447 - acc: 0.7164\n",
            "Epoch 45/100\n",
            " - 1s - loss: 1.8473 - acc: 0.7132\n",
            "Epoch 46/100\n",
            " - 1s - loss: 1.8465 - acc: 0.7186\n",
            "Epoch 47/100\n",
            " - 1s - loss: 1.8463 - acc: 0.7214\n",
            "Epoch 48/100\n",
            " - 1s - loss: 1.8450 - acc: 0.7193\n",
            "Epoch 49/100\n",
            " - 1s - loss: 1.8456 - acc: 0.7259\n",
            "Epoch 50/100\n",
            " - 1s - loss: 1.8449 - acc: 0.7188\n",
            "Epoch 51/100\n",
            " - 1s - loss: 1.8452 - acc: 0.7176\n",
            "Epoch 52/100\n",
            " - 1s - loss: 1.8434 - acc: 0.7196\n",
            "Epoch 53/100\n",
            " - 1s - loss: 1.8468 - acc: 0.7216\n",
            "Epoch 54/100\n",
            " - 1s - loss: 1.8453 - acc: 0.7211\n",
            "Epoch 55/100\n",
            " - 1s - loss: 1.8441 - acc: 0.7224\n",
            "Epoch 56/100\n",
            " - 1s - loss: 1.8443 - acc: 0.7271\n",
            "Epoch 57/100\n",
            " - 1s - loss: 1.8435 - acc: 0.7234\n",
            "Epoch 58/100\n",
            " - 1s - loss: 1.8432 - acc: 0.7200\n",
            "Epoch 59/100\n",
            " - 1s - loss: 1.8438 - acc: 0.7238\n",
            "Epoch 60/100\n",
            " - 1s - loss: 1.8436 - acc: 0.7219\n",
            "Epoch 61/100\n",
            " - 1s - loss: 1.8425 - acc: 0.7267\n",
            "Epoch 62/100\n",
            " - 1s - loss: 1.8436 - acc: 0.7215\n",
            "Epoch 63/100\n",
            " - 1s - loss: 1.8421 - acc: 0.7239\n",
            "Epoch 64/100\n",
            " - 1s - loss: 1.8438 - acc: 0.7209\n",
            "Epoch 65/100\n",
            " - 1s - loss: 1.8409 - acc: 0.7218\n",
            "Epoch 66/100\n",
            " - 1s - loss: 1.8432 - acc: 0.7214\n",
            "Epoch 67/100\n",
            " - 1s - loss: 1.8445 - acc: 0.7192\n",
            "Epoch 68/100\n",
            " - 1s - loss: 1.8438 - acc: 0.7248\n",
            "Epoch 69/100\n",
            " - 1s - loss: 1.8410 - acc: 0.7216\n",
            "Epoch 70/100\n",
            " - 1s - loss: 1.8391 - acc: 0.7214\n",
            "Epoch 71/100\n",
            " - 1s - loss: 1.8456 - acc: 0.7221\n",
            "Epoch 72/100\n",
            " - 1s - loss: 1.8435 - acc: 0.7244\n",
            "Epoch 73/100\n",
            " - 1s - loss: 1.8442 - acc: 0.7212\n",
            "Epoch 74/100\n",
            " - 1s - loss: 1.8409 - acc: 0.7254\n",
            "Epoch 75/100\n",
            " - 1s - loss: 1.8423 - acc: 0.7231\n",
            "Epoch 76/100\n",
            " - 1s - loss: 1.8424 - acc: 0.7243\n",
            "Epoch 77/100\n",
            " - 1s - loss: 1.8426 - acc: 0.7234\n",
            "Epoch 78/100\n",
            " - 1s - loss: 1.8432 - acc: 0.7169\n",
            "Epoch 79/100\n",
            " - 1s - loss: 1.8415 - acc: 0.7205\n",
            "Epoch 80/100\n",
            " - 1s - loss: 1.8414 - acc: 0.7242\n",
            "Epoch 81/100\n",
            " - 1s - loss: 1.8402 - acc: 0.7245\n",
            "Epoch 82/100\n",
            " - 1s - loss: 1.8409 - acc: 0.7240\n",
            "Epoch 83/100\n",
            " - 1s - loss: 1.8401 - acc: 0.7261\n",
            "Epoch 84/100\n",
            " - 1s - loss: 1.8410 - acc: 0.7222\n",
            "Epoch 85/100\n",
            " - 1s - loss: 1.8424 - acc: 0.7199\n",
            "Epoch 86/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 1s - loss: 1.8426 - acc: 0.7239\n",
            "Epoch 87/100\n",
            " - 1s - loss: 1.8385 - acc: 0.7235\n",
            "Epoch 88/100\n",
            " - 1s - loss: 1.8424 - acc: 0.7227\n",
            "Epoch 89/100\n",
            " - 1s - loss: 1.8420 - acc: 0.7238\n",
            "Epoch 90/100\n",
            " - 1s - loss: 1.8427 - acc: 0.7209\n",
            "Epoch 91/100\n",
            " - 1s - loss: 1.8415 - acc: 0.7218\n",
            "Epoch 92/100\n",
            " - 1s - loss: 1.8371 - acc: 0.7221\n",
            "Epoch 93/100\n",
            " - 1s - loss: 1.8421 - acc: 0.7239\n",
            "Epoch 94/100\n",
            " - 1s - loss: 1.8420 - acc: 0.7291\n",
            "Epoch 95/100\n",
            " - 1s - loss: 1.8422 - acc: 0.7193\n",
            "Epoch 96/100\n",
            " - 1s - loss: 1.8418 - acc: 0.7209\n",
            "Epoch 97/100\n",
            " - 1s - loss: 1.8399 - acc: 0.7235\n",
            "Epoch 98/100\n",
            " - 1s - loss: 1.8409 - acc: 0.7190\n",
            "Epoch 99/100\n",
            " - 1s - loss: 1.8378 - acc: 0.7264\n",
            "Epoch 100/100\n",
            " - 1s - loss: 1.8402 - acc: 0.7253\n",
            "4000/4000 [==============================] - 0s 37us/step\n",
            "[[-0.09601549  0.22609544  0.48284888]\n",
            " [-0.29245228  0.2798578  -0.28653333]\n",
            " [ 0.15583347 -0.20106408  0.48346913]\n",
            " [ 0.08943075 -0.6722016   0.09471997]\n",
            " [-0.21997407  0.00940921  0.36042416]\n",
            " [ 0.04642984 -0.36303613  0.262235  ]\n",
            " [-0.25169837  0.19933245  0.365799  ]\n",
            " [-0.1275247   0.1217664  -0.28439382]\n",
            " [ 0.46517944 -0.14778377  0.1656929 ]\n",
            " [ 0.23125724 -0.10933703  0.44456762]]\n",
            "[array([-0.3010909 ,  0.44611016,  0.6743065 ], dtype=float32), array([-0.20129949,  0.19695945,  0.04894563], dtype=float32), array([0.39652237, 0.2556701 , 0.9983507 ], dtype=float32), array([ 0.17510441, -0.73801386,  0.24791189], dtype=float32), array([-0.5265462 ,  0.16677967,  0.6441164 ], dtype=float32), array([-0.08020746, -0.16649598, -0.17534085], dtype=float32), array([-0.08588097,  0.2540514 ,  0.06176935], dtype=float32), array([-0.43910477,  0.04692475, -0.00365461], dtype=float32), array([ 1.0192825 ,  0.02583425, -0.5902829 ], dtype=float32), array([0.24491951, 0.02684591, 0.36311415], dtype=float32)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# neural net\\nnet = nn.Sequential(\\n    nn.Linear(6, 192),\\n    nn.ReLU(),\\n    nn.Linear(192, 32),\\n    nn.ReLU(),\\n    nn.Linear(32, 1)\\n)\\noptimizer = optim.Adam(net.parameters(), lr=1E-4)\\ncriterion = nn.MSELoss()\\nnet.train()\\n\\n# train\\nfor epoch in trange(epochs * Nsamples): # using trange instead of range produces a progressbar\\n    # randomly select a sample\\n    isample = np.random.randint(0, Nsamples)\\n    y0 = C(ys[isample])\\n \\n    # calculate neural net output\\n    X = V(Xs[isample]) - torch.cat([origin, origin])\\n    z = torch.sum(net(X))\\n    # z = torch.sum(1 / torch.norm(X[:,:3], dim=1) / torch.norm(X[:,3:], dim=1) * 1e4)  # this is 100% accurate\\n    y = torch.autograd.grad(z, origin, create_graph=True)[0]\\n \\n    # and perform train step\\n    loss = criterion(y,y0)\\n    optimizer.zero_grad()   # suggested trick\\n    loss.backward()\\n    optimizer.step()\\n \\n    if epoch % 1000 == 0:\\n        tqdm.write('%s\\t%s\\t%s\\t%s' %(epoch, N(loss)[0], N(y0)[0], N(y)[0]))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "egCjM-xYP8mn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IEVK-KFxi5Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0940e8b1-5283-4b2d-e3e6-8853f3deb804"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXRh0DPiZRyG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "metadata": {
        "id": "t9ALbbpmY9rm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c5ee74e3-f88b-4660-dcba-354d09fa5659"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "8.350230318000058\n",
            "GPU (s):\n",
            "0.1842791589999706\n",
            "GPU speedup over CPU: 45x\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}